{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugBmkxiWJ6ru"
      },
      "source": [
        "# Question 1 : Model Training for both regression and classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QEBl0_bKSxn"
      },
      "source": [
        "**Dataset : Wine Quality**\n",
        "\n",
        "**URL : https://archive.ics.uci.edu/dataset/186/wine+quality**\n",
        "\n",
        "**Method for classification**\n",
        "\n",
        "*   Decision Tree\n",
        "*   Random Forests\n",
        "*   KNN\n",
        "*   Naive Bayes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFdFKoD3L8lA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"c:/Users/Htet Aung/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gcVinjH9MYr_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('winequality-red.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSh7IYl1Nb81"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVGt3t-kNjQ6"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.isnull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSmJSE3hNutn"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.corr(), vmin = 0, vmax = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iin9W60LNz-U"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwYIo8MzN6nd"
      },
      "outputs": [],
      "source": [
        "num_columns = len(df.columns)\n",
        "cols = 3  # Number of columns for the subplots\n",
        "rows = (num_columns + cols - 1) // cols  # Calculate number of rows needed\n",
        "\n",
        "plt.figure(figsize=(15, 5 * rows))  # Adjust the height based on the number of rows\n",
        "\n",
        "for i, column in enumerate(df.columns):\n",
        "    plt.subplot(rows, cols, i + 1)  # Create a subplot for each column\n",
        "    sns.histplot(data=df, x=column, kde=True, bins=30, color='#DDA0DD')\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXL_Byv4O3oO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6 * rows))  # Here we increase the figure height and width\n",
        "\n",
        "for i, column in enumerate(df.columns[:-1]):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    sns.scatterplot(data=df, x=column, y='quality', hue='quality', size='quality', sizes=(50, 300), alpha=0.7, legend=False)\n",
        "    plt.title(f'Scatter Plot of {column} vs Quality')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Quality')\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout(pad=3.0)  # Adjust layout to prevent overlap and add padding\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK6DodkyPy76"
      },
      "source": [
        "**Do sweeter wines receive better ratings?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_deZN6tNP1Z_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='residual sugar', y='quality', hue='quality',\n",
        "                palette='viridis', size='quality', sizes=(50, 300), alpha=0.7, legend='full')\n",
        "\n",
        "plt.title('Scatter Plot of Residual Sugar vs Quality', fontsize=16)\n",
        "plt.xlabel('Residual Sugar', fontsize=14)\n",
        "plt.ylabel('Quality', fontsize=14)\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.legend(title='Quality', bbox_to_anchor=(1.05, 1), loc='upper left')  # Adjust legend position\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyX1uZCQQRxh"
      },
      "outputs": [],
      "source": [
        "mean_sugar = df['residual sugar'].mean()\n",
        "meanofhighsugar = df[df['residual sugar'] > mean_sugar].quality.mean()\n",
        "meanoflowsugar = df[df['residual sugar'] < mean_sugar].quality.mean()\n",
        "\n",
        "print(\"Average quality of wines with low sugar\", meanoflowsugar)\n",
        "print(\"Average quality of wines with high sugar\", meanofhighsugar)\n",
        "\n",
        "plt.bar(x = ['High Sugar', 'Low Sugar'], height = [meanofhighsugar, meanoflowsugar])\n",
        "plt.ylim((5,6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Zo7DrMQnb0"
      },
      "outputs": [],
      "source": [
        "mean_acidity = df['fixed acidity'].mean()\n",
        "meanofhighalc = df[df['fixed acidity'] > mean_acidity].quality.mean()\n",
        "meanoflowalc = df[df['fixed acidity'] < mean_acidity].quality.mean()\n",
        "\n",
        "print(\"Average quality of wines with low acidity\", meanoflowalc)\n",
        "print(\"Average quality of wines with high acidity\", meanofhighalc)\n",
        "\n",
        "plt.bar(x = ['High Acidity', 'Low Acidity'], height = [meanofhighalc, meanoflowalc])\n",
        "plt.ylim((5, 6))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErYmUg0HQ1-c"
      },
      "source": [
        "Which wines are produced more? (according to quality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbNbd_wbQ4D-"
      },
      "outputs": [],
      "source": [
        "df['quality'].value_counts().sort_index().plot(kind='bar', xlabel='Quality', ylabel='quantity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhktZtvPRPtO"
      },
      "outputs": [],
      "source": [
        "#Converting data to binary classification\n",
        "\n",
        "df[\"quality\"] = np.where(df[\"quality\"] >= 7, 1, 0)\n",
        "df.quality.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m5xQ4wcRU7I"
      },
      "outputs": [],
      "source": [
        "#Model Creation and Training\n",
        "\n",
        "models = ['KNN Classifier', 'Decision Tree', 'Random forest', 'Naive Bayes']\n",
        "score =[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbQibzswSCa1"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M3uZPRSIAf"
      },
      "source": [
        "**Splitting Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VE5gygtSPET"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df.drop('quality', axis = 1), df['quality'], test_size = 0.2, random_state=42)\n",
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgXP1ri-Sw5c"
      },
      "source": [
        "**Scaling data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-rn8RS8S099"
      },
      "outputs": [],
      "source": [
        "Y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS6lCisyS9ul"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmA7DSIHTD0z"
      },
      "source": [
        "\n",
        "\n",
        "> KNN Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E-qgp1VTeTg"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors = 13)\n",
        "knn_model.fit(scaled_X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPthhwQnTuj3"
      },
      "outputs": [],
      "source": [
        "knn_pred = knn_model.predict(scaled_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lf0BcXLT2Sp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "print(classification_report(Y_test, knn_pred))\n",
        "#print(confusion_matrix(Y_test, knn_pred))\n",
        "print(accuracy_score(Y_test, knn_pred))\n",
        "\n",
        "score.append(accuracy_score(Y_test, knn_pred))\n",
        "print(score)\n",
        "#print(np.mean(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx_RQExuUXjl"
      },
      "source": [
        "\n",
        "\n",
        "> **Decision Tree**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S0w11GuUf1O"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Classifier Model Training\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_dtree = DecisionTreeClassifier()\n",
        "model_dtree.fit(scaled_X_train, Y_train)\n",
        "dtree_pred = model_dtree.predict(scaled_X_test)\n",
        "print(classification_report(Y_test, dtree_pred))\n",
        "\n",
        "\n",
        "score.append(accuracy_score(Y_test, dtree_pred))\n",
        "print(score)\n",
        "#print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHaHtAhFVRZh"
      },
      "outputs": [],
      "source": [
        "#Feature Importance of Decision Tree Classifier Model\n",
        "\n",
        "plt.figure(figsize = (10, 5))\n",
        "plt.bar(df.columns[:-1], model_dtree.feature_importances_)\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBChvJqZVh91"
      },
      "source": [
        "\n",
        "\n",
        "> **Random Forest**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT_QTVhnVons"
      },
      "outputs": [],
      "source": [
        "#Model training of Random Forest Classifier Method\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_classifier = RandomForestClassifier(max_features = 3, n_estimators = 169, bootstrap = True)\n",
        "random_forest_classifier.fit(scaled_X_train, Y_train)\n",
        "rfc_pred = random_forest_classifier.predict(scaled_X_test)\n",
        "print(classification_report(Y_test, rfc_pred))\n",
        "\n",
        "score.append(accuracy_score(Y_test, rfc_pred))\n",
        "print(score)\n",
        "#print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5TdFkqWT-Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Naive Bayes Classification\n",
        "naive_bayes_model = GaussianNB()\n",
        "naive_bayes_model.fit(scaled_X_train, Y_train)\n",
        "nb_pred = naive_bayes_model.predict(scaled_X_test)\n",
        "\n",
        "print(classification_report(Y_test, nb_pred))\n",
        "#print(confusion_matrix(Y_test, nb_pred))\n",
        "print(accuracy_score(Y_test, nb_pred))\n",
        "\n",
        "score.append(accuracy_score(Y_test, nb_pred))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqCwZVtCWsdp"
      },
      "outputs": [],
      "source": [
        "print(score)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, score, color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Model Accuracy\")\n",
        "\n",
        "# Display the plot\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization\n",
        "plt.show()\n",
        "\n",
        "highest_score_index = np.argmax(score)  # Here we get the index of the highest score\n",
        "highest_score_model = models[highest_score_index]  # Get the corresponding model name from the models dictionary\n",
        "highest_score_value = score[highest_score_index]  # Get the highest score value\n",
        "\n",
        "# Print the highest score model and its score\n",
        "print(f\"The model with the highest score is: {highest_score_model} with an accuracy of {highest_score_value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmf46WU1bx3h"
      },
      "source": [
        "**Regression Model**\n",
        "\n",
        "For regression model for this dataset we use the following models\n",
        "\n",
        "*   Linear Regression\n",
        "*   Lasso Regression\n",
        "*   Decision Tree Regression\n",
        "*   Random Forest Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvk8r9hcSSw"
      },
      "outputs": [],
      "source": [
        "score_reg = []\n",
        "models_reg = ['Linear Regression', 'Lasso Regression', 'Decision Tree Regression', 'Random Forest Regression']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Vlw1PZdE4j"
      },
      "source": [
        "\n",
        "\n",
        "> **Linear Regression**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqgFIwB-cckm"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "linear_regression_model = LinearRegression()\n",
        "\n",
        "linear_regression_model.fit(X_train, Y_train)\n",
        "\n",
        "linear_regression_predictions = linear_regression_model.predict(X_test)\n",
        "\n",
        "# Evaluation of the model\n",
        "mse = mean_squared_error(Y_test, linear_regression_predictions)\n",
        "r2 = r2_score(Y_test, linear_regression_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "score_reg.append(r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP2mw1jcdPVh"
      },
      "source": [
        "\n",
        "\n",
        "> **Lasso Regression**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn9p14_KdVnw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_regression_model = Lasso(alpha=0.1)  # You can adjust the alpha parameter\n",
        "\n",
        "lasso_regression_model.fit(X_train, Y_train)\n",
        "\n",
        "lasso_regression_predictions = lasso_regression_model.predict(X_test)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(Y_test, lasso_regression_predictions)\n",
        "r2 = r2_score(Y_test, lasso_regression_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "score_reg.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hzfSLTjd5qP"
      },
      "source": [
        "\n",
        "\n",
        "> **Decision Tree Regression**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwAF1rk9eIX1"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Decision Tree Regression Model Training\n",
        "decision_tree_regression_model = DecisionTreeRegressor()\n",
        "decision_tree_regression_model.fit(X_train, Y_train)\n",
        "decision_tree_regression_predictions = decision_tree_regression_model.predict(X_test)\n",
        "\n",
        "# Evaluation of the decison tree regression model\n",
        "mse = mean_squared_error(Y_test, decision_tree_regression_predictions)\n",
        "r2 = r2_score(Y_test, decision_tree_regression_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "score_reg.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYitIf4he3NU"
      },
      "source": [
        "\n",
        "\n",
        "> **Random Forest Regression**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20P1Fz7Ce-FU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Random Forest Regression Model Training\n",
        "random_forest_regression_model = RandomForestRegressor()\n",
        "random_forest_regression_model.fit(X_train, Y_train)\n",
        "random_forest_regression_predictions = random_forest_regression_model.predict(X_test)\n",
        "\n",
        "# Evaluation of the random forest regression model\n",
        "mse = mean_squared_error(Y_test, random_forest_regression_predictions)\n",
        "r2 = r2_score(Y_test, random_forest_regression_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "score_reg.append(r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjJDn4E0fU05"
      },
      "source": [
        "**Comparison of each regression model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evXBSSo5fa2y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models_reg, score_reg, color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Regression Models\")\n",
        "plt.ylabel(\"R-squared Score\")\n",
        "plt.title(\"Regression Model Performance\")\n",
        "\n",
        "# Display the plot\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization\n",
        "plt.show()\n",
        "\n",
        "highest_score_index_reg = np.argmax(score_reg)  # Here we get the index of the highest score\n",
        "highest_score_model_reg = models_reg[highest_score_index_reg]  # Get the corresponding model name from the models dictionary\n",
        "highest_score_value_reg = score_reg[highest_score_index_reg]  # Get the highest score value\n",
        "\n",
        "# Print the highest score model and its score\n",
        "print(f\"The regression model with the highest R-squared is: {highest_score_model_reg} with an R-squared of {highest_score_value_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa6GbFJQgI8Z"
      },
      "source": [
        "# Question 2 : Overfitting and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUM0y93qhoWk"
      },
      "outputs": [],
      "source": [
        "# Decision Tree with potential overfitting\n",
        "model_dtree_overfit = DecisionTreeClassifier()\n",
        "model_dtree_overfit.fit(X_train, Y_train)\n",
        "dtree_pred_overfit = model_dtree_overfit.predict(X_test)\n",
        "print(\"Decision Tree (Potential Overfitting):\")\n",
        "print(classification_report(Y_test, dtree_pred_overfit))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, dtree_pred_overfit))\n",
        "\n",
        "# Decision Tree with Pruning to prevent overfitting\n",
        "model_dtree_pruned = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5)\n",
        "model_dtree_pruned.fit(X_train, Y_train)\n",
        "dtree_pred_pruned = model_dtree_pruned.predict(X_test)\n",
        "print(\"\\nDecision Tree (Pruned):\")\n",
        "print(classification_report(Y_test, dtree_pred_pruned))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, dtree_pred_pruned))\n",
        "\n",
        "# Random Forest with potential overfitting\n",
        "random_forest_classifier_overfit = RandomForestClassifier(n_estimators=1000)\n",
        "random_forest_classifier_overfit.fit(X_train, Y_train)\n",
        "rfc_pred_overfit = random_forest_classifier_overfit.predict(X_test)\n",
        "print(\"\\nRandom Forest (Potential Overfitting):\")\n",
        "print(classification_report(Y_test, rfc_pred_overfit))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, rfc_pred_overfit))\n",
        "\n",
        "# Random Forest with Regularization to prevent overfitting\n",
        "random_forest_classifier_regularized = RandomForestClassifier(n_estimators=100, max_features=3, max_depth=5)\n",
        "random_forest_classifier_regularized.fit(X_train, Y_train)\n",
        "rfc_pred_regularized = random_forest_classifier_regularized.predict(X_test)\n",
        "print(\"\\nRandom Forest (Regularized):\")\n",
        "print(classification_report(Y_test, rfc_pred_regularized))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, rfc_pred_regularized))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXJR74xzkIIM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "data = pd.read_csv(\"winequality-red.csv\")\n",
        "data.head()\n",
        "\n",
        "# Feature selection (assuming 'quality' is the target variable)\n",
        "X = data.drop('quality', axis=1)\n",
        "Y = data['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Decision Tree with potential overfitting\n",
        "model_dtree_overfit = DecisionTreeClassifier()\n",
        "model_dtree_overfit.fit(X_train, Y_train)\n",
        "dtree_pred_overfit = model_dtree_overfit.predict(X_test)\n",
        "\n",
        "# Decision Tree with Pruning\n",
        "model_dtree_pruned = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5)\n",
        "model_dtree_pruned.fit(X_train, Y_train)\n",
        "dtree_pred_pruned = model_dtree_pruned.predict(X_test)\n",
        "\n",
        "# Random Forest with potential overfitting\n",
        "random_forest_classifier_overfit = RandomForestClassifier(n_estimators=1000)\n",
        "random_forest_classifier_overfit.fit(X_train, Y_train)\n",
        "rfc_pred_overfit = random_forest_classifier_overfit.predict(X_test)\n",
        "\n",
        "# Random Forest with Regularization\n",
        "random_forest_classifier_regularized = RandomForestClassifier(n_estimators=100, max_features=3, max_depth=5)\n",
        "random_forest_classifier_regularized.fit(X_train, Y_train)\n",
        "rfc_pred_regularized = random_forest_classifier_regularized.predict(X_test)\n",
        "\n",
        "# Visualization of results\n",
        "models = ['DTree Overfit', 'DTree Pruned', 'RFC Overfit', 'RFC Regularized']\n",
        "accuracies = [\n",
        "    accuracy_score(Y_test, dtree_pred_overfit),\n",
        "    accuracy_score(Y_test, dtree_pred_pruned),\n",
        "    accuracy_score(Y_test, rfc_pred_overfit),\n",
        "    accuracy_score(Y_test, rfc_pred_regularized)\n",
        "]\n",
        "\n",
        "# Bar chart for accuracy comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracies, color=['red', 'green', 'orange', 'blue'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison on Wine Quality Dataset')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Decision Tree Overfit\n",
        "cm_overfit = confusion_matrix(Y_test, dtree_pred_overfit)\n",
        "ConfusionMatrixDisplay(cm_overfit).plot(ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title('Decision Tree (Potential Overfitting)')\n",
        "\n",
        "# Decision Tree Pruned\n",
        "cm_pruned = confusion_matrix(Y_test, dtree_pred_pruned)\n",
        "ConfusionMatrixDisplay(cm_pruned).plot(ax=axes[1], cmap='Greens')\n",
        "axes[1].set_title('Decision Tree (Pruned)')\n",
        "\n",
        "# Random Forest Overfit\n",
        "cm_rfc_overfit = confusion_matrix(Y_test, rfc_pred_overfit)\n",
        "ConfusionMatrixDisplay(cm_rfc_overfit).plot(ax=axes[2], cmap='Oranges')\n",
        "axes[2].set_title('Random Forest (Potential Overfitting)')\n",
        "\n",
        "# Random Forest Regularized\n",
        "cm_rfc_regularized = confusion_matrix(Y_test, rfc_pred_regularized)\n",
        "ConfusionMatrixDisplay(cm_rfc_regularized).plot(ax=axes[3], cmap='Purples')\n",
        "axes[3].set_title('Random Forest (Regularized)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RyGhDU0sAxU"
      },
      "source": [
        "# Question 3 : Feature selection and correlation analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAAfffZqsoMk"
      },
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "correlation_matrix = df.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Wine Quality Features')\n",
        "plt.show()\n",
        "\n",
        "# Identify features highly correlated with 'quality'\n",
        "correlation_with_quality = correlation_matrix['quality'].sort_values(ascending=False)\n",
        "print(\"Correlation with Quality:\")\n",
        "print(correlation_with_quality)\n",
        "\n",
        "# Select features with a correlation above a certain threshold (e.g., 0.2)\n",
        "selected_features = correlation_with_quality[abs(correlation_with_quality) > 0.2].index.tolist()\n",
        "print(\"\\nSelected Features:\")\n",
        "print(selected_features)\n",
        "\n",
        "# Create a new dataset with only the selected features\n",
        "df_selected = df[selected_features]\n",
        "\n",
        "# Split the data into training and testing sets using the selected features\n",
        "X_train_selected, X_test_selected, Y_train_selected, Y_test_selected = train_test_split(\n",
        "    df_selected.drop('quality', axis=1), df_selected['quality'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Training models using the selected features (example with Random Forest)\n",
        "random_forest_classifier_selected = RandomForestClassifier()\n",
        "random_forest_classifier_selected.fit(X_train_selected, Y_train_selected)\n",
        "rfc_pred_selected = random_forest_classifier_selected.predict(X_test_selected)\n",
        "print(\"\\nRandom Forest with Selected Features:\")\n",
        "print(classification_report(Y_test_selected, rfc_pred_selected))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test_selected, rfc_pred_selected))\n",
        "\n",
        "# Training and evaluating Decision Tree with selected features\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "decision_tree_classifier.fit(X_train_selected, Y_train_selected)\n",
        "dt_pred_selected = decision_tree_classifier.predict(X_test_selected)\n",
        "\n",
        "print(\"\\nDecision Tree with Selected Features:\")\n",
        "print(classification_report(Y_test_selected, dt_pred_selected))\n",
        "print(\"Accuracy:\", accuracy_score(Y_test_selected, dt_pred_selected))\n",
        "\n",
        "# Training and evaluating Linear Regression (for regression task)\n",
        "linear_regression_model = LinearRegression()\n",
        "linear_regression_model.fit(X_train_selected, Y_train_selected)\n",
        "lin_pred_selected = linear_regression_model.predict(X_test_selected)\n",
        "\n",
        "print(\"\\nLinear Regression with Selected Features:\")\n",
        "print(\"Mean Squared Error:\", mean_squared_error(Y_test_selected, lin_pred_selected))\n",
        "print(\"R-squared:\", r2_score(Y_test_selected, lin_pred_selected))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
